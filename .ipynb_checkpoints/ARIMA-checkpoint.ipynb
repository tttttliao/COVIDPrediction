{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first read in our train and test data. We assume that all the data are storedd as csv files in a separate `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_TRAIN_BY_STATE = \"./data/train_by_state/\"\n",
    "\n",
    "train_data = pd.read_csv('./data/train.csv', engine='python').filter(items=['ID', 'Province_State', 'Date', 'Confirmed', 'Deaths'])\n",
    "test_data = pd.read_csv('./data/test.csv', engine='python') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will potentially have a different model for every state, for convenience, we separate the train data into respective states to accelerate the learning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alabama' 'Alaska' 'Arizona' 'Arkansas' 'California' 'Colorado'\n",
      " 'Connecticut' 'Delaware' 'Florida' 'Georgia' 'Hawaii' 'Idaho' 'Illinois'\n",
      " 'Indiana' 'Iowa' 'Kansas' 'Kentucky' 'Louisiana' 'Maine' 'Maryland'\n",
      " 'Massachusetts' 'Michigan' 'Minnesota' 'Mississippi' 'Missouri' 'Montana'\n",
      " 'Nebraska' 'Nevada' 'New Hampshire' 'New Jersey' 'New Mexico' 'New York'\n",
      " 'North Carolina' 'North Dakota' 'Ohio' 'Oklahoma' 'Oregon' 'Pennsylvania'\n",
      " 'Rhode Island' 'South Carolina' 'South Dakota' 'Tennessee' 'Texas' 'Utah'\n",
      " 'Vermont' 'Virginia' 'Washington' 'West Virginia' 'Wisconsin' 'Wyoming']\n"
     ]
    }
   ],
   "source": [
    "# Get list of state names\n",
    "states_names = np.unique(np.array([train_data['Province_State']]))\n",
    "assert(len(states_names) == 50)\n",
    "print(states_names)\n",
    "\n",
    "def split_train_data_by_state(train_data):\n",
    "    for state in states_names:\n",
    "        state_data = train_data[train_data['Province_State'] == state]\n",
    "        csv_name = DIR_TRAIN_BY_STATE + state + \".csv\"\n",
    "        state_data.to_csv(csv_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we only want to do this if we haven't done it already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DIR_TRAIN_BY_STATE):\n",
    "    os.mkdir(DIR_TRAIN_BY_STATE)\n",
    "    \n",
    "if not len(os.listdir(DIR_TRAIN_BY_STATE)):\n",
    "    split_train_data_by_state(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the best hyperparameters, we generate candidates to do a grid search for the one with best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4), (1, 1, 5), (1, 1, 6), (1, 1, 7), (2, 1, 1), (2, 1, 2), (2, 1, 3), (2, 1, 4), (2, 1, 5), (2, 1, 6), (2, 1, 7), (3, 1, 1), (3, 1, 2), (3, 1, 3), (3, 1, 4), (3, 1, 5), (3, 1, 6), (3, 1, 7), (4, 1, 1), (4, 1, 2), (4, 1, 3), (4, 1, 4), (4, 1, 5), (4, 1, 6), (4, 1, 7)]\n",
      "[(1, 1, 1, 12), (1, 1, 2, 12), (1, 1, 3, 12), (1, 1, 4, 12), (1, 1, 5, 12), (1, 1, 6, 12), (1, 1, 7, 12), (2, 1, 1, 12), (2, 1, 2, 12), (2, 1, 3, 12), (2, 1, 4, 12), (2, 1, 5, 12), (2, 1, 6, 12), (2, 1, 7, 12), (3, 1, 1, 12), (3, 1, 2, 12), (3, 1, 3, 12), (3, 1, 4, 12), (3, 1, 5, 12), (3, 1, 6, 12), (3, 1, 7, 12), (4, 1, 1, 12), (4, 1, 2, 12), (4, 1, 3, 12), (4, 1, 4, 12), (4, 1, 5, 12), (4, 1, 6, 12), (4, 1, 7, 12)]\n"
     ]
    }
   ],
   "source": [
    "p = range(1, 5)\n",
    "d = [1]\n",
    "q = range(1, 8)\n",
    "pdq_candidates = list(itertools.product(p, d, q))\n",
    "seasonal_pdq_candidates = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n",
    "print(pdq_candidates)\n",
    "print(seasonal_pdq_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(pred, gt):\n",
    "    ape = np.abs(pred - gt) / np.abs(gt)\n",
    "    return np.mean(ape) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(discard_date=\"04-12-2020\", split_date=\"08-06-2020\"):\n",
    "    valid_data = train_data[(train_data['Date'] >= split_date) & (train_data['Date'] <= \"08-31-2020\")]\n",
    "    train_set = train_data[(train_data['Date'] >= discard_date) & (train_data['Date'] < split_date)]\n",
    "#     print(train_set.head())\n",
    "\n",
    "    valid_confirmed_dict = {}    \n",
    "    valid_death_dict = {}\n",
    "    train_confirmed_dict = {}    \n",
    "    train_death_dict = {}\n",
    "\n",
    "    for state in states_names:\n",
    "        state_valid = valid_data[valid_data[\"Province_State\"] == state]\n",
    "        state_train = train_set[train_set[\"Province_State\"] == state]\n",
    "\n",
    "        state_valid_c = np.array(state_valid[\"Confirmed\"].tolist())\n",
    "        state_valid_d = np.array(state_valid[\"Deaths\"].tolist())\n",
    "        valid_confirmed_dict[state] = state_valid_c\n",
    "        valid_death_dict[state] = state_valid_d\n",
    "        \n",
    "        state_train_c = np.array(state_train[\"Confirmed\"].tolist())\n",
    "        state_train_d = np.array(state_train[\"Deaths\"].tolist())\n",
    "        train_confirmed_dict[state] = state_train_c\n",
    "        train_death_dict[state] = state_train_d\n",
    "\n",
    "    return train_confirmed_dict, train_death_dict, valid_confirmed_dict, valid_death_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_confirmed_dict, train_death_dict, valid_confirmed_dict, valid_death_dict = train_valid_split()\n",
    "train_confirmed_dict[\"Alabama\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_arima():\n",
    "    predictions = []\n",
    "    for state in states_names:\n",
    "        state_data = pd.read_csv(DIR_TRAIN_BY_STATE + state + \".csv\")\n",
    "        \n",
    "        valid_errors_confirmed =[]\n",
    "        valid_errors_death =[]\n",
    "\n",
    "        \n",
    "        mape_confirmed = 1e7\n",
    "        pdq_confirmed = None\n",
    "        model_confirmed = None\n",
    "\n",
    "        mape_death = 1e7\n",
    "        pdq_death = None\n",
    "        model_death = None\n",
    "\n",
    "        for pdq in pdq_candidates:\n",
    "            mod = ARIMA(train_confirmed_dict[state], order=pdq, enforce_stationarity=False)\n",
    "            f = mod.fit()\n",
    "            pred_c = f.predict(start=train_confirmed_dict[state].shape[0], end=train_confirmed_dict[state].shape[0] + valid_confirmed_dict[state].shape[0] - 1)\n",
    "            error = mape(np.array(pred_c.tolist()), valid_confirmed_dict[state])\n",
    "            if error < mape_confirmed:\n",
    "                print(\"Updating param: \", error, pdq)\n",
    "                mape_confirmed = error\n",
    "                pdq_confirmed = pdq\n",
    "                valid_errors_confirmed.append(error)\n",
    "\n",
    "        print(\"Best parameter for \", state, \"'s confirmed is PDQ: \", pdq_confirmed)\n",
    "        model_confirmed = ARIMA(state_data['Confirmed'], order=pdq_confirmed, enforce_stationarity=False)\n",
    "        plt.plot(valid_errors_confirmed,label='Validation Error of Confirmed for ' + state)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('error')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        for pdq in pdq_candidates:\n",
    "            mod = ARIMA(train_death_dict[state], order=pdq, enforce_stationarity=False)\n",
    "\n",
    "            f = mod.fit()\n",
    "            pred_d = f.predict(start=train_death_dict[state].shape[0], end=train_death_dict[state].shape[0] + valid_death_dict[state].shape[0] - 1)\n",
    "            error = mape(np.array(pred_d.tolist()), valid_death_dict[state])\n",
    "            if error < mape_death:\n",
    "                print(\"Updating param: \", error, pdq)\n",
    "                mape_death = error\n",
    "                pdq_death = pdq\n",
    "                valid_errors_death.append(error)\n",
    "        \n",
    "        print(\"Best parameter for \", state, \"'s death is PDQ: \", pdq_death)\n",
    "        model_death = ARIMA(state_data['Deaths'], order=pdq_death, enforce_stationarity=False)\n",
    "        plt.plot(valid_errors_death,label='Validation Error of Death for ' + state)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('error')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        fit_confirmed = model_confirmed.fit()\n",
    "        predict_confirmed = fit_confirmed.predict(start=142, end=167)\n",
    "        fit_death = model_death.fit()\n",
    "\n",
    "        predict_death = fit_death.predict(start=142, end=167)\n",
    "        predictions.append((np.around(predict_confirmed, decimals=0), np.around(predict_death, decimals=0)))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_seasonal_arima():\n",
    "    predictions = []\n",
    "    for state in states_names:\n",
    "        state_data = pd.read_csv(DIR_TRAIN_BY_STATE + state + \".csv\")\n",
    "\n",
    "        mape_confirmed = 1e7\n",
    "        pdq_confirmed = None\n",
    "        seasonal_pdq_confirmed = None\n",
    "        model_confirmed = None\n",
    "\n",
    "        mape_death = 1e7\n",
    "        pdq_death = None\n",
    "        seasonal_pdq_death = None\n",
    "        model_death = None\n",
    "\n",
    "        for pdq in pdq_candidates:\n",
    "            for seasonal_pdq in seasonal_pdq_candidates:\n",
    "                try:\n",
    "                    mod = SARIMAX(train_confirmed_dict[state], order=pdq, seasonal_order=seasonal_pdq,\n",
    "                                  enforce_stationarity=False, enforce_invertibility=False)\n",
    "\n",
    "                    f = mod.fit(disp=False, method='powell')\n",
    "                    pred_c = f.predict(start=train_confirmed_dict[state].shape[0], end=train_confirmed_dict[state].shape[0] + valid_confirmed_dict[state].shape[0] - 1)\n",
    "                    error = mape(np.array(pred_c.tolist()), valid_confirmed_dict[state])\n",
    "                    if error < mape_confirmed:\n",
    "                        print(\"Updating param: \", error, pdq_confirmed, seasonal_pdq_confirmed)\n",
    "                        mape_confirmed = error\n",
    "                        pdq_confirmed = pdq\n",
    "                        seasonal_pdq_confirmed = seasonal_pdq\n",
    "                except:\n",
    "                    continue\n",
    "       \n",
    "        print(\"Best parameter for \", state, \"'s confirmed is PDQ: \", pdq_confirmed, \" and Seasonal PDQ: \", seasonal_pdq_confirmed)\n",
    "        model_confirmed = SARIMAX(state_data['Confirmed'], order=pdq_confirmed, seasonal_order=seasonal_pdq_confirmed,\n",
    "                                  enforce_stationarity=False, enforce_invertibility=False)\n",
    "\n",
    "        for pdq in pdq_candidates:\n",
    "                    for seasonal_pdq in seasonal_pdq_candidates:\n",
    "                        try:\n",
    "                            mod = SARIMAX(train_death_dict[state], order=pdq, seasonal_order=seasonal_pdq,\n",
    "                                          enforce_stationarity=False, enforce_invertibility=False)\n",
    "                            f = mod.fit(disp=False, method='powell')\n",
    "                            pred_d = f.predict(start=train_death_dict[state].shape[0], end=train_death_dict[state].shape[0] + valid_death_dict[state].shape[0] - 1)\n",
    "                            error = mape(np.array(pred_d.tolist()), valid_death_dict[state])\n",
    "                            if error < mape_death:\n",
    "                                print(\"Updating param: \", error, pdq, seasonal_pdq)\n",
    "                                mape_death = error\n",
    "                                pdq_death = pdq\n",
    "                                seasonal_pdq_death = seasonal_pdq\n",
    "                        except:\n",
    "                            continue\n",
    "        \n",
    "        print(\"Best parameter for \", state, \"'s deaths is PDQ: \", pdq_death, \" and Seasonal PDQ: \", seasonal_pdq_death)\n",
    "        model_death = SARIMAX(state_data['Deaths'],order=pdq_death, seasonal_order=seasonal_pdq_death,\n",
    "                                  enforce_stationarity=False, enforce_invertibility=False)\n",
    "\n",
    "        fit_c = model_confirmed.fit(disp=False, method='powell')\n",
    "        y_pred_confirmed = fit_c.predict(start=142, end=167)\n",
    "        fit_d = model_death.fit(disp=False, method='powell')\n",
    "\n",
    "        y_pred_deaths = fit_d.predict(start=142, end=167)\n",
    "        predictions.append((np.around(y_pred_confirmed, decimals=0), np.around(y_pred_deaths, decimals=0)))\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating param:  3.2340492434643973 None None\n",
      "Updating param:  2.866304217248371 (1, 1, 1) (1, 1, 1, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonyl\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for seasonal ARMA. All parameters except for variances will be set to zeros.\n",
      "  warn('Too few observations to estimate starting parameters%s.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating param:  2.632335155589353 (1, 1, 1) (1, 1, 2, 12)\n"
     ]
    }
   ],
   "source": [
    "predictions = run_arima()\n",
    "# predictions = run_seasonal_arima()\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_submission = test_data.sort_values([\"Province_State\", \"Date\"])\n",
    "confirmed = []\n",
    "deaths = []\n",
    "for i in range(50):\n",
    "    confirmed.append(predictions[i][0].astype(int).tolist())\n",
    "    deaths.append(predictions[i][1].astype(int).tolist())\n",
    "\n",
    "confirmed = list(itertools.chain.from_iterable(confirmed))\n",
    "deaths = list(itertools.chain.from_iterable(deaths))\n",
    "\n",
    "test_submission.loc[:, \"Confirmed\"] = confirmed\n",
    "test_submission.loc[:, \"Deaths\"] = deaths\n",
    "\n",
    "test_submission = test_submission.sort_index().filter(items=['ForecastID', 'Confirmed', 'Deaths'])\n",
    "test_submission.to_csv(\"Team15_arima.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasonal Arima was meant to apply seasonal effect to better capture the trend in COVID 19 growth. However, in practice add seasonal parameters massively increased running time and did not produce better results. The best score we were able to achieve is about 2.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
