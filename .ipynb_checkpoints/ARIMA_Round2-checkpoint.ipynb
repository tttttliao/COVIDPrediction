{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first read in our train and test data. We assume that all the data are storedd as csv files in a separate `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11245</th>\n",
       "      <td>11245</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>11-22-2020</td>\n",
       "      <td>217796</td>\n",
       "      <td>3938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11246</th>\n",
       "      <td>11246</td>\n",
       "      <td>Washington</td>\n",
       "      <td>11-22-2020</td>\n",
       "      <td>141260</td>\n",
       "      <td>2619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11247</th>\n",
       "      <td>11247</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>11-22-2020</td>\n",
       "      <td>40478</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11248</th>\n",
       "      <td>11248</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>11-22-2020</td>\n",
       "      <td>376238</td>\n",
       "      <td>3150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11249</th>\n",
       "      <td>11249</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>11-22-2020</td>\n",
       "      <td>28169</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID Province_State        Date  Confirmed  Deaths\n",
       "11245  11245       Virginia  11-22-2020     217796    3938\n",
       "11246  11246     Washington  11-22-2020     141260    2619\n",
       "11247  11247  West Virginia  11-22-2020      40478     662\n",
       "11248  11248      Wisconsin  11-22-2020     376238    3150\n",
       "11249  11249        Wyoming  11-22-2020      28169     176"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR_TRAIN_BY_STATE = \"./data/train_by_state/\"\n",
    "\n",
    "train_data = pd.read_csv('./data/train_round2.csv', engine='python').filter(items=['ID', 'Province_State', 'Date', 'Confirmed', 'Deaths'])\n",
    "test_data = pd.read_csv('./data/test_round2.csv', engine='python')\n",
    "train_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will potentially have a different model for every state, for convenience, we separate the train data into respective states to accelerate the learning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alabama' 'Alaska' 'Arizona' 'Arkansas' 'California' 'Colorado'\n",
      " 'Connecticut' 'Delaware' 'Florida' 'Georgia' 'Hawaii' 'Idaho' 'Illinois'\n",
      " 'Indiana' 'Iowa' 'Kansas' 'Kentucky' 'Louisiana' 'Maine' 'Maryland'\n",
      " 'Massachusetts' 'Michigan' 'Minnesota' 'Mississippi' 'Missouri' 'Montana'\n",
      " 'Nebraska' 'Nevada' 'New Hampshire' 'New Jersey' 'New Mexico' 'New York'\n",
      " 'North Carolina' 'North Dakota' 'Ohio' 'Oklahoma' 'Oregon' 'Pennsylvania'\n",
      " 'Rhode Island' 'South Carolina' 'South Dakota' 'Tennessee' 'Texas' 'Utah'\n",
      " 'Vermont' 'Virginia' 'Washington' 'West Virginia' 'Wisconsin' 'Wyoming']\n"
     ]
    }
   ],
   "source": [
    "# Get list of state names\n",
    "states_names = np.unique(np.array([train_data['Province_State']]))\n",
    "assert(len(states_names) == 50)\n",
    "print(states_names)\n",
    "\n",
    "def split_train_data_by_state(train_data):\n",
    "    for state in states_names:\n",
    "        state_data = train_data[train_data['Province_State'] == state]\n",
    "        csv_name = DIR_TRAIN_BY_STATE + state + \".csv\"\n",
    "        state_data.to_csv(csv_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we only want to do this if we haven't done it already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DIR_TRAIN_BY_STATE):\n",
    "    os.mkdir(DIR_TRAIN_BY_STATE)\n",
    "    \n",
    "if not len(os.listdir(DIR_TRAIN_BY_STATE)):\n",
    "    split_train_data_by_state(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the best hyperparameters, we generate candidates to do a grid search for the one with best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4), (1, 1, 5), (1, 1, 6), (1, 1, 7), (2, 1, 1), (2, 1, 2), (2, 1, 3), (2, 1, 4), (2, 1, 5), (2, 1, 6), (2, 1, 7), (3, 1, 1), (3, 1, 2), (3, 1, 3), (3, 1, 4), (3, 1, 5), (3, 1, 6), (3, 1, 7), (4, 1, 1), (4, 1, 2), (4, 1, 3), (4, 1, 4), (4, 1, 5), (4, 1, 6), (4, 1, 7)]\n",
      "[(1, 1, 1, 12), (1, 1, 2, 12), (1, 1, 3, 12), (1, 1, 4, 12), (1, 1, 5, 12), (1, 1, 6, 12), (1, 1, 7, 12), (2, 1, 1, 12), (2, 1, 2, 12), (2, 1, 3, 12), (2, 1, 4, 12), (2, 1, 5, 12), (2, 1, 6, 12), (2, 1, 7, 12), (3, 1, 1, 12), (3, 1, 2, 12), (3, 1, 3, 12), (3, 1, 4, 12), (3, 1, 5, 12), (3, 1, 6, 12), (3, 1, 7, 12), (4, 1, 1, 12), (4, 1, 2, 12), (4, 1, 3, 12), (4, 1, 4, 12), (4, 1, 5, 12), (4, 1, 6, 12), (4, 1, 7, 12)]\n"
     ]
    }
   ],
   "source": [
    "p = range(1, 5)\n",
    "d = [1]\n",
    "q = range(1, 8)\n",
    "pdq_candidates = list(itertools.product(p, d, q))\n",
    "seasonal_pdq_candidates = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n",
    "print(pdq_candidates)\n",
    "print(seasonal_pdq_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(pred, gt):\n",
    "    ape = np.abs(pred - gt) / np.abs(gt)\n",
    "    return np.mean(ape) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split():\n",
    "    valid_data = train_data[(train_data['Date'] >= \"11-01-2020\") & (train_data['Date'] <= \"11-22-2020\")]\n",
    "    train_set = train_data[(train_data['Date'] < \"11-01-2020\")]\n",
    "    valid_confirmed_dict = {}    \n",
    "    valid_death_dict = {}\n",
    "    train_confirmed_dict = {}    \n",
    "    train_death_dict = {}\n",
    "\n",
    "    for state in states_names:\n",
    "        state_valid = valid_data[valid_data[\"Province_State\"] == state]\n",
    "        state_train = train_set[train_set[\"Province_State\"] == state]\n",
    "\n",
    "        state_valid_c = np.array(state_valid[\"Confirmed\"].tolist())\n",
    "        state_valid_d = np.array(state_valid[\"Deaths\"].tolist())\n",
    "        valid_confirmed_dict[state] = state_valid_c\n",
    "        valid_death_dict[state] = state_valid_d\n",
    "        \n",
    "        state_train_c = np.array(state_train[\"Confirmed\"].tolist())\n",
    "        state_train_d = np.array(state_train[\"Deaths\"].tolist())\n",
    "        train_confirmed_dict[state] = state_train_c\n",
    "        train_death_dict[state] = state_train_d\n",
    "\n",
    "    return train_confirmed_dict, train_death_dict, valid_confirmed_dict, valid_death_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_confirmed_dict, train_death_dict, valid_confirmed_dict, valid_death_dict = train_valid_split()\n",
    "train_confirmed_dict[\"Alabama\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_arima():\n",
    "    predictions = []\n",
    "    for state in states_names:\n",
    "        state_data = pd.read_csv(DIR_TRAIN_BY_STATE + state + \".csv\")\n",
    "        \n",
    "        mape_confirmed = 1e7\n",
    "        pdq_confirmed = None\n",
    "        model_confirmed = None\n",
    "\n",
    "        mape_death = 1e7\n",
    "        pdq_death = None\n",
    "        model_death = None\n",
    "\n",
    "        for pdq in pdq_candidates:\n",
    "            mod = ARIMA(train_confirmed_dict[state], order=pdq, enforce_stationarity=False)\n",
    "            f = mod.fit(method=\"statespace\")\n",
    "            pred_c = f.predict(start=train_confirmed_dict[state].shape[0], end=train_confirmed_dict[state].shape[0] + valid_confirmed_dict[state].shape[0] - 1)\n",
    "            error = mape(np.array(pred_c.tolist()), valid_confirmed_dict[state])\n",
    "            if error < mape_confirmed:\n",
    "                print(\"Updating param: \", error, pdq)\n",
    "                mape_confirmed = error\n",
    "                pdq_confirmed = pdq\n",
    "\n",
    "        print(\"Best parameter for \", state, \"'s confirmed is PDQ: \", pdq_confirmed)\n",
    "        model_confirmed = ARIMA(state_data['Confirmed'], order=pdq_confirmed, enforce_stationarity=False)\n",
    "\n",
    "        for pdq in pdq_candidates:\n",
    "            mod = ARIMA(train_death_dict[state], order=pdq, enforce_stationarity=False)\n",
    "\n",
    "            f = mod.fit(method=\"statespace\")\n",
    "            pred_d = f.predict(start=train_death_dict[state].shape[0], end=train_death_dict[state].shape[0] + valid_death_dict[state].shape[0] - 1)\n",
    "            error = mape(np.array(pred_d.tolist()), valid_death_dict[state])\n",
    "            if error < mape_death:\n",
    "                print(\"Updating param: \", error, pdq)\n",
    "                mape_death = error\n",
    "                pdq_death = pdq\n",
    "        \n",
    "        print(\"Best parameter for \", state, \"'s death is PDQ: \", pdq_death)\n",
    "        model_death = ARIMA(state_data['Deaths'], order=pdq_death, enforce_stationarity=False)\n",
    "\n",
    "        fit_confirmed = model_confirmed.fit(method=\"statespace\")\n",
    "        predict_confirmed = fit_confirmed.predict(start=225, end=245)\n",
    "        fit_death = model_death.fit(method=\"statespace\")\n",
    "\n",
    "        predict_death = fit_death.predict(start=225, end=245)\n",
    "        predictions.append((np.around(predict_confirmed, decimals=0), np.around(predict_death, decimals=0)))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_seasonal_arima():\n",
    "    predictions = []\n",
    "    for state in states_names:\n",
    "        state_data = pd.read_csv(DIR_TRAIN_BY_STATE + state + \".csv\")\n",
    "\n",
    "        mape_confirmed = 1e7\n",
    "        pdq_confirmed = None\n",
    "        seasonal_pdq_confirmed = None\n",
    "        model_confirmed = None\n",
    "\n",
    "        mape_death = 1e7\n",
    "        pdq_death = None\n",
    "        seasonal_pdq_death = None\n",
    "        model_death = None\n",
    "\n",
    "        for pdq in pdq_candidates:\n",
    "            try:\n",
    "                mod = SARIMAX(train_confirmed_dict[state], order=pdq,# seasonal_order=seasonal_pdq,\n",
    "                              enforce_stationarity=False, enforce_invertibility=False)\n",
    "\n",
    "                f = mod.fit(disp=False, method='powell')\n",
    "                pred_c = f.predict(start=train_confirmed_dict[state].shape[0], end=train_confirmed_dict[state].shape[0] + valid_confirmed_dict[state].shape[0] - 1)\n",
    "                error = mape(np.array(pred_c.tolist()), valid_confirmed_dict[state])\n",
    "                if error < mape_confirmed:\n",
    "                    print(\"Updating confirmed param: \", error, pdq)\n",
    "                    mape_confirmed = error\n",
    "                    pdq_confirmed = pdq\n",
    "            except:\n",
    "                continue\n",
    "       \n",
    "        print(\"Best parameter for \", state, \"'s confirmed is PDQ: \", pdq_confirmed)\n",
    "        model_confirmed = SARIMAX(state_data['Confirmed'], order=pdq_confirmed,# seasonal_order=seasonal_pdq_confirmed,\n",
    "                                  enforce_stationarity=False, enforce_invertibility=False)\n",
    "\n",
    "        for pdq in pdq_candidates:\n",
    "                try:\n",
    "                    mod = SARIMAX(train_death_dict[state], order=pdq,# seasonal_order=seasonal_pdq,\n",
    "                                  enforce_stationarity=False, enforce_invertibility=False)\n",
    "                    f = mod.fit(disp=False, method='powell')\n",
    "                    pred_d = f.predict(start=train_death_dict[state].shape[0], end=train_death_dict[state].shape[0] + valid_death_dict[state].shape[0] - 1)\n",
    "                    error = mape(np.array(pred_d.tolist()), valid_death_dict[state])\n",
    "                    if error < mape_death:\n",
    "                        print(\"Updating death param: \", error, pdq)\n",
    "                        mape_death = error\n",
    "                        pdq_death = pdq\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        print(\"Best parameter for \", state, \"'s deaths is PDQ: \", pdq_death)\n",
    "        model_death = SARIMAX(state_data['Deaths'],order=pdq_death,# seasonal_order=seasonal_pdq_death,\n",
    "                                  enforce_stationarity=False, enforce_invertibility=False)\n",
    "\n",
    "        fit_c = model_confirmed.fit(disp=False, method='powell')\n",
    "        y_pred_confirmed = fit_c.predict(start=239, end=245)\n",
    "        fit_d = model_death.fit(disp=False, method='powell')\n",
    "\n",
    "        y_pred_deaths = fit_d.predict(start=239, end=245)\n",
    "        predictions.append((np.around(y_pred_confirmed, decimals=0), np.around(y_pred_deaths, decimals=0)))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating confirmed param:  0.6774556777337909 (1, 1, 1)\n",
      "Updating confirmed param:  0.6769914314694178 (1, 1, 3)\n",
      "Updating confirmed param:  0.6751151804847361 (2, 1, 1)\n",
      "Updating confirmed param:  0.6683758927423774 (2, 1, 2)\n",
      "Updating confirmed param:  0.6675154099749339 (3, 1, 3)\n",
      "Updating confirmed param:  0.641834075566313 (4, 1, 3)\n",
      "Best parameter for  Alabama 's confirmed is PDQ:  (4, 1, 3)\n",
      "Updating death param:  2.0474580561732436 (1, 1, 1)\n",
      "Updating death param:  1.9806573099945355 (1, 1, 2)\n",
      "Updating death param:  1.9785982262792188 (1, 1, 3)\n",
      "Updating death param:  1.881865264048907 (2, 1, 2)\n",
      "Updating death param:  1.6226314745785977 (3, 1, 4)\n",
      "Best parameter for  Alabama 's deaths is PDQ:  (3, 1, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(239    261321.0\n",
       "  240    262865.0\n",
       "  241    264413.0\n",
       "  242    265964.0\n",
       "  243    267518.0\n",
       "  244    269077.0\n",
       "  245    270638.0\n",
       "  dtype: float64,\n",
       "  239    5208.0\n",
       "  240    5249.0\n",
       "  241    5291.0\n",
       "  242    5333.0\n",
       "  243    5375.0\n",
       "  244    5417.0\n",
       "  245    5460.0\n",
       "  dtype: float64)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions = run_arima()\n",
    "predictions = run_seasonal_arima()\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_submission = test_data.sort_values([\"Province_State\", \"Date\"])\n",
    "confirmed = []\n",
    "deaths = []\n",
    "for i in range(50):\n",
    "    confirmed.append(predictions[i][0].astype(int).tolist())\n",
    "    deaths.append(predictions[i][1].astype(int).tolist())\n",
    "\n",
    "confirmed = list(itertools.chain.from_iterable(confirmed))\n",
    "deaths = list(itertools.chain.from_iterable(deaths))\n",
    "\n",
    "test_submission.loc[:, \"Confirmed\"] = confirmed\n",
    "test_submission.loc[:, \"Deaths\"] = deaths\n",
    "\n",
    "test_submission = test_submission.sort_index().filter(items=['ForecastID', 'Confirmed', 'Deaths'])\n",
    "test_submission.to_csv(\"Team15_arima_round2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
